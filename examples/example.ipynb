{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with QuickTake\n",
    "\n",
    "How to call the QuickTake class and use the methods/APIs.\n",
    "\n",
    "Once instantiated all methods are available to use. \n",
    "\n",
    "*_Args_*\n",
    "\n",
    "The `new_init` argument is used to either instantiate a new model or use an existing model if found in the cache.\n",
    "\n",
    "---\n",
    "```\n",
    ": 24 Aug 2023\n",
    ": zach wolpe\n",
    ": zachcolinwolpe@gmail.com\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance of `QuickTake` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# example images\n",
    "img = './data/random/IMG_5993.jpeg'\n",
    "img = './data/random/dave_cen.png'\n",
    "img = './data/random/dave.png'\n",
    "\n",
    "# to avoid distractions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int module\n",
    "from quicktake import QuickTake\n",
    "qt = QuickTake()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# YoloV5: Object Detection\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and tranform if required\n",
    "img_        = qt.read_image(img)\n",
    "transform_  = qt.inference_transforms(input_size_=64)\n",
    "transform_(img_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw pixels\n",
    "results_, time_ = qt.yolov5(image=img_, new_init=True)\n",
    "results_, time_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "results_.print() # Results, see: results.save(), help(results) # or .show()\n",
    "points = results_.pandas().xyxy[0] # get raw results data (points of interest)\n",
    "print('\\n\\nObjects detected:\\n', points.to_markdown(), '\\n\\n') # print formatted results\n",
    "results_.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gender & Age Determination\n",
    "----\n",
    "\n",
    "Gender and age determination are trained on faces. They work fine on a larger image, however the will fail to make multiple predictions in the case of multiple faces in a single image.\n",
    "\n",
    "The APIs intended use is to chain models, thus the `gender` and `age` models are designed to take the output of the `yolo` model as input.\n",
    "\n",
    "See the source code of `QuickTake.launchStream()` if a more detailed example is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Yolo to extract faces\n",
    "qt      = QuickTake()\n",
    "frame   = qt.read_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points\n",
    "for _label, x0,y0,x1,y1, colour, thickness, results, res_df, age_, gender_ in qt.yolo_loop(frame):\n",
    "    _label = QuickTake.generate_yolo_label(_label)\n",
    "    QuickTake.add_block_to_image(frame, _label, x0,y0,x1,y1, colour=colour, thickness=thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# outside of notebook\n",
    "# cv2.imshow(\"test\", frame)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# inside of notebook\n",
    "#   - In Pillow, the order of colors is assumed to be RGB (red, green, blue).\n",
    "#   - As we are using Image.fromarray() of PIL module, we need to convert BGR to RGB.\n",
    "\n",
    "img_view = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) # Converting BGR to RGB\n",
    "display(Image.fromarray(img_view))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Webcam Stream\n",
    "----\n",
    "\n",
    "Launch stream\n",
    "- best not in a notebook is an interactive terminal is required to kill the session.\n",
    "- kill the session with 'q' or 'esc' key or 'ctrl + c' in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quicktake import QuickTake\n",
    "qt = QuickTake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "qt.launchStream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Individual Model Calls\n",
    "----\n",
    "\n",
    "Finally, the `QuickTake` class can be used to call each model individually.\n",
    "\n",
    "_*Note*_: Each model returns the results `results_` as well as the runtime `time_`.\n",
    "\n",
    "Example inputs parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size  = 64\n",
    "X           = torch.randn(3, 224, 224, requires_grad=False)#.to(tgp.device)\n",
    "X_yolo      = torch.randn(1, 3, 224, 224, requires_grad=False)#.to(tgp.device)\n",
    "image_path  = './data/random/dave.png'\n",
    "image_paths = './data/random/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process a torch.Tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_, time_ = QL.gender(X, new_init=True)\n",
    "results_, time_ = QL.yolov5(X_yolo, new_init=True)\n",
    "results_, time_ = QL.age(X, new_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process a single image (path):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_, time_ = QL.gender(image_path, new_init=True)\n",
    "results_, time_ = QL.yolov5(image_path, new_init=True)\n",
    "results_, time_ = QL.age(image_path, new_init=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process a directory of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_, time_ = QL.gender(image_paths, new_init=True)\n",
    "results_, time_ = QL.yolov5(image_paths, new_init=True)\n",
    "results_, time_ = QL.age(image_paths, new_init=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-it",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
